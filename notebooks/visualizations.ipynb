{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "\n",
    "OKABE_ITO_SCALE = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"]\n",
    "sns.set_palette(sns.color_palette(OKABE_ITO_SCALE))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "FIG_DIR = Path(\"../figures\")\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "DATA_MODALITIES = [\"graph\",\"numerical\", \"binary\", \"nominal\", \"timeseries\", \"NLP\", \"cv\", \"multiomics\", \"seq_of_sets\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize how results differ on numerical datasets as we change size of the vector passed to the RSIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_n():\n",
    "    def plot(title):\n",
    "        g = sns.relplot(data=df, x=\"max_n\", y=\"auc\", hue=\"type\", marker=\"o\", col=\"dataset_name\", col_wrap=2, \n",
    "                facet_kws={'sharey': False, 'sharex': False}, kind=\"line\")\n",
    "        g.set(ylim=(0, 1))\n",
    "        g.fig.suptitle(f'influence of max_n on AUC for {dist} distance')\n",
    "        g.set(xscale=\"log\")\n",
    "        g.fig.subplots_adjust(top=0.9)\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(20, 10)\n",
    "        fig.savefig(FIG_DIR / title)\n",
    "        plt.close()\n",
    "\n",
    "    dfs = []\n",
    "    for dist in [\"manhattan\", \"chebyshev\", \"cosine\"]:\n",
    "        df1 = pd.read_csv(RESULTS_DIR / f\"max_n/{dist}_projection-fixed.csv\")\n",
    "        df2 = pd.read_csv(RESULTS_DIR / f\"max_n/{dist}_projection-not_fixed.csv\")\n",
    "        df1.drop(columns=[f\"{dist}_projection-fixed\"], inplace=True)\n",
    "        df2.drop(columns=[f\"{dist}_projection-not_fixed\"], inplace=True)\n",
    "\n",
    "        df1[\"type\"] = dist + \"_fixed\"\n",
    "        df2[\"type\"] = dist + \"_not_fixed\"\n",
    "\n",
    "        df = pd.concat([df1, df2], ignore_index=True)\n",
    "        plot(f\"max_n_{dist}.png\")\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    plot(\"max_n_together.png\")\n",
    "\n",
    "plot_max_n()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results of changing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_plot(csv_path, parameter_name, col_wrap=4):\n",
    "    results = pd.read_csv(csv_path)\n",
    "    sns.set_palette(sns.color_palette(OKABE_ITO_SCALE))\n",
    "    g = sns.relplot(data=results, x=parameter_name, y=\"auc\", hue=\"clf\", marker=\"o\", col=\"dataset_name\", col_wrap=col_wrap, \n",
    "                facet_kws={'sharey': False, 'sharex': False}, kind=\"line\")\n",
    "    g.set(ylim=(0, 1))\n",
    "    g.fig.suptitle(f'influence of {parameter_name} on AUC')\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 10)\n",
    "    fig.savefig(FIG_DIR / f\"{parameter_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "sensitivity_plot(\"../results/max_samples.csv\", \"max_samples\", 5)\n",
    "sensitivity_plot(\"../results/n_estimators.csv\", \"n_estimators\", 5)\n",
    "sensitivity_plot(\"../results/selected_obj_ratio.csv\", \"obj_ratio\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing results of trying different distances combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n_dists(data_type, n=7):\n",
    "    path = RESULTS_DIR / \"selected_distances\"  / f\"{data_type}.csv\"\n",
    "    if not path.exists():\n",
    "        return\n",
    "\n",
    "    results = pd.read_csv(path)\n",
    "    if_default = pd.read_csv(RESULTS_DIR / \"if_default.csv\")\n",
    "    grouped = results.groupby(['dataset_name', 'distances'])\n",
    "    grouped_mean = grouped.auc.mean().reset_index()\n",
    "    datasets = grouped_mean.dataset_name.unique()\n",
    "    fig, ax = plt.subplots(1, len(datasets),  figsize=(10 * len(datasets), 10))\n",
    "    \n",
    "    if not isinstance(ax, np.ndarray):\n",
    "        ax = [ax]\n",
    "\n",
    "    for i in range(0, len(datasets)):\n",
    "        dataset_name = grouped_mean.dataset_name.unique()[i]\n",
    "        if_score = if_default[if_default.dataset_name == dataset_name].auc.values\n",
    "        if_mean = if_score.mean()\n",
    "        top_n_dists = grouped_mean[grouped_mean.dataset_name == dataset_name].sort_values(by=['auc'], ascending=False).head(n).distances.values\n",
    "\n",
    "        to_plot = results[results.dataset_name == dataset_name]\n",
    "        to_plot = to_plot[to_plot.distances.isin(top_n_dists)]\n",
    "        \n",
    "        to_plot[\"index\"] = to_plot[\"distances\"].apply(lambda x: list(top_n_dists).index(x))\n",
    "        to_plot = to_plot.sort_values(by=['index'])\n",
    "        sns.barplot(data=to_plot, x=\"distances\", y=\"auc\", ax=ax[i])\n",
    "        ax[i].set_title(dataset_name)\n",
    "        ax[i].tick_params(labelrotation=45)\n",
    "        ax[i].axhline(if_mean, ls='-', color='red', lw=3, label='IF default')\n",
    "        \n",
    "    fig.suptitle(f\"Different distance functions combinations comparisons - {data_type}\")\n",
    "    fig.savefig(FIG_DIR / f\"top_n_dists_{data_type}.png\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modality in DATA_MODALITIES:\n",
    "    plot_top_n_dists(modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save scores obtained by different distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def generate_ranking(data_type, file):\n",
    "    path =  RESULTS_DIR / \"selected_distances\"  / f\"{data_type}.csv\"\n",
    "    if not path.exists():\n",
    "        return\n",
    "    f.write(f\"{data_type.upper()}===========\\n\")\n",
    "\n",
    "    num_df = pd.read_csv(path)\n",
    "    algorithms_rank = defaultdict(lambda: 0)\n",
    "    for dataset_name in np.unique(num_df.dataset_name):\n",
    "        scores = num_df[num_df.dataset_name == dataset_name]\n",
    "        scores = scores.sort_values(by=\"auc\", ascending=False)\n",
    "        n_combinations = scores.shape[0]\n",
    "        \n",
    "        points_achieved = 1 / np.arange(1, n_combinations+1)\n",
    "        for i, candidate in enumerate(scores.distances):\n",
    "            candidates = candidate.split(\"_\")\n",
    "            for c in candidates:\n",
    "                algorithms_rank[c] += points_achieved[i] / len(candidates)\n",
    "    normalizer = sum(algorithms_rank.values())\n",
    "    for name, score in sorted(algorithms_rank.items(), key=lambda x: x[1], reverse=True):\n",
    "        f.write(f\"{name} -> {score / normalizer}\\n\")\n",
    "\n",
    "with open(FIG_DIR / \"distances_ranking.txt\", \"w\") as f:\n",
    "    f.write(\"SCORE OF DISTANCES FOR EACH MODALITY TESTED IN SENSITIVITY ANALYSIS\\n\")\n",
    "    for modality in DATA_MODALITIES:\n",
    "        generate_ranking(modality, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing results of final experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in [\"graph\",\"numerical\", \"binary\", \"nominal\", \"timeseries\", \"NLP\", \"cv\", \"multiomics\", \"seq_of_sets\"]:\n",
    "    df = pd.read_csv(f'../results/experiments/{dtype}.csv')\n",
    "    sns.barplot(data=df, x=\"dataset_name\", y=\"auc\", hue=\"clf\")\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15, 8)\n",
    "    fig.savefig(FIG_DIR / f\"final_results_{dtype}.png\")\n",
    "    plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "def read_all_data(subset = [\"binary\", \"nominal\", \"timeseries\",\"cv\", \"multiomics\",\"graph\", \"nlp\", \"numerical\", \"seq_of_sets\"], table_form = False):\n",
    "    dfs = []\n",
    "    for dtype in subset:\n",
    "        dfs.append(pd.read_csv(f'../results/experiments/{dtype}.csv'))\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    if table_form:\n",
    "        df = df.groupby(['dataset_name', 'clf']).mean(numeric_only=True).reset_index()\n",
    "        df = pd.pivot_table(df, values='auc', index=['dataset_name'], columns=['clf'])\n",
    "    return df\n",
    "\n",
    "df_table = read_all_data(table_form=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing standard deviation of scores obtained by different algoritghms on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.plot(kind='bar', figsize=(25, 10))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"std of AUC for each dataset and classifier\")\n",
    "plt.savefig(FIG_DIR / \"std_auc.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving results of Friedman, Posthoc and mean rank of every algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\mambaforge\\envs\\risf\\lib\\site-packages\\scipy\\stats\\_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "c:\\Users\\sebas\\mambaforge\\envs\\risf\\lib\\site-packages\\scipy\\stats\\_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "c:\\Users\\sebas\\mambaforge\\envs\\risf\\lib\\site-packages\\scipy\\stats\\_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_rank(df):\n",
    "    ranks = []\n",
    "    for row in df.iterrows():\n",
    "        id_, row = row\n",
    "        ranks.append(row.rank(ascending=False).values)\n",
    "\n",
    "    return pd.Series(np.mean(ranks, axis=0), index=df.columns)\n",
    "\n",
    "def calculate_posthoc(df):\n",
    "    posthoc = sp.posthoc_wilcoxon(df.T.to_numpy())\n",
    "    posthoc.columns = df.columns\n",
    "    posthoc.index = df.columns.values\n",
    "    return posthoc\n",
    "\n",
    "def calculate_friedman(df):\n",
    "    return friedmanchisquare(*[df[c] for c in df.columns])\n",
    "\n",
    "#Analysis for entire data\n",
    "friedman = calculate_friedman(df_table)\n",
    "posthoc = calculate_posthoc(df_table)\n",
    "mean_rank = calculate_mean_rank(df_table)\n",
    "\n",
    "#Every subset separately\n",
    "subsets = [DATA_MODALITIES, *[[x] for x in DATA_MODALITIES], list(set(DATA_MODALITIES) - {\"numerical\"})]\n",
    "df_tests = []\n",
    "posthoc_idx = posthoc.index + \"_post\"\n",
    "for set_of_modalities in subsets:\n",
    "    df = read_all_data(set_of_modalities, table_form=True)\n",
    "    try:\n",
    "        posthoc = (calculate_posthoc(df) < 0.05).sum()\n",
    "        posthoc.index = posthoc_idx\n",
    "    except:\n",
    "        posthoc = pd.Series([0] * len(df.columns), index=posthoc_idx)\n",
    "\n",
    "    row = {**calculate_mean_rank(df).to_dict(), **posthoc.to_dict()}\n",
    "    row[\"fried_passed\"] = calculate_friedman(df).pvalue < 0.05\n",
    "\n",
    "    if set_of_modalities == DATA_MODALITIES:\n",
    "        row[\"idx\"] = \"all\"\n",
    "    elif len(set_of_modalities) == len(DATA_MODALITIES) - 1:\n",
    "        row[\"idx\"] = \"complex\"\n",
    "    else:\n",
    "        row[\"idx\"] = \"_\".join(set_of_modalities)\n",
    "    df_tests.append(row)\n",
    "\n",
    "separately = pd.DataFrame(df_tests).set_index(\"idx\")\n",
    "\n",
    "with open(FIG_DIR / \"tests.txt\", \"w\") as f:\n",
    "    f.write(f\"p-value whole data: {friedman.pvalue}\\n\\n\\n\")\n",
    "    f.write(f\"statistic whole data: {friedman.statistic}\\n\\n\\n\")\n",
    "    f.write(f\"mean rank whole data: {mean_rank}\\n\\n\\n\")\n",
    "    f.write(f\"posthoc whole data:\\n{posthoc}\\n\\n\\n\")\n",
    "    f.write(f\"separately:\\n{separately}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of results of original IF implementation and RSIF in a \"IF\" mode. RSIF works alike IF when we set `dummy_projection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../results/numerical_dummy_dist.csv\").groupby([\"clf\", \"dataset_name\"]).auc.mean().reset_index().pivot(index=\"dataset_name\", columns=\"clf\", values=\"auc\")\n",
    "df['diff'] = df['IForest'] - df['RSIF']\n",
    "df.to_csv(FIG_DIR / \"IF_implementation_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of number of holdouts needed for the mean and standard deviation to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_DIR / \"20holdouts_sensitivity_datasets.csv\")\n",
    "df = df.set_index([\"dataset_name\", \"clf\"]).sort_index()\n",
    "data_to_plot = []\n",
    "for dataset_name, clf in np.unique(df.index.to_numpy()):\n",
    "    results = df.loc[(dataset_name, clf)].reset_index()\n",
    "    for n_holdouts in range(1, len(results)+1):\n",
    "        samples = results.iloc[:n_holdouts, 3].to_numpy()\n",
    "        data_to_plot.append({\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"clf\": clf,\n",
    "            \"n_holdouts\": n_holdouts,\n",
    "            \"auc_std\": samples.std(),\n",
    "            \"auc_mean\": samples.mean(),\n",
    "            \"samples\": samples\n",
    "        })\n",
    "df_plt = pd.DataFrame(data_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (25,5)\n",
    "for stat in [\"mean\", \"std\"]:\n",
    "    g = sns.relplot(data=df_plt,  x=\"n_holdouts\", y=f\"auc_{stat}\", hue=\"clf\", marker=\"o\", col=\"dataset_name\", col_wrap=5, \n",
    "                    facet_kws={'sharey': False, 'sharex': False}, kind=\"line\")\n",
    "    g.set(ylim=(0, 1 if stat == \"mean\" else 0.2))\n",
    "    g.fig.suptitle(f'{stat.upper()} score vs number of repeated holdouts fore every dataset and classifier', fontsize=16)\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(FIG_DIR / f\"{stat}_holdouts_convergence.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing results of hyperparameters search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_results = read_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_ids = []\n",
    "previous_comb = None\n",
    "for row in df_final_results.iterrows():\n",
    "    row = row[1]\n",
    "    comb = row[\"dataset_name\"] + row[\"clf\"]\n",
    "    if comb != previous_comb:\n",
    "        previous_comb = comb\n",
    "        fold_ids += list(range(10))\n",
    "\n",
    "df_final_results['fold'] = fold_ids\n",
    "df_final_results = pd.pivot_table(df_final_results, columns=\"clf\", values=\"auc\", index=['dataset_name', 'dataset_type', 'fold']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from risf.distance_functions import GraphDist\n",
    "from risf.distance import SelectiveDistance\n",
    "def get_shortcut(dist):\n",
    "    if isinstance(dist, SelectiveDistance):\n",
    "        return dist.projection_func.__name__\n",
    "    if isinstance(dist, GraphDist):\n",
    "        return dist.distance.__class__.__name__\n",
    "    return dist.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "def get_single_result(file, all_entries = None):\n",
    "    clf = file.stem.split(\"_\")[0] + \"_tuned\"\n",
    "    dataset = \"_\".join(file.stem.split(\"_\")[1:])\n",
    "    with open(file, \"rb\") as f:\n",
    "        best_distances = pickle.load(open(file, \"rb\"))\n",
    "        for fold_id, distances in best_distances.items():\n",
    "            result = {\"clf\": clf}\n",
    "            distances_list = []\n",
    "            n_dist = min([len(distances), 4])\n",
    "            for n in range(n_dist):\n",
    "                if n == 0:\n",
    "                    result[f\"auc\"] = distances[n][0]\n",
    "                distances_list.append(\"++\".join([get_shortcut(d) for d in distances[n][1]]))\n",
    "            result[\"fold\"] = fold_id\n",
    "            result[f'distances_{clf}'] = distances_list\n",
    "            result[\"dataset_name\"] = dataset\n",
    "            if all_entries is not None:\n",
    "                all_entries.append(result)\n",
    "            else:\n",
    "                return result\n",
    "            \n",
    "\n",
    "entries = []\n",
    "for file in Path(\"../best_distances/\").iterdir():\n",
    "    if \"RSIF\" in file.stem and (\"ovarian\" in file.stem or \"breast\" in file.stem or \"rosmap\" in file.stem):\n",
    "        continue\n",
    "    if \"LOF\" not in file.stem:\n",
    "        get_single_result(file, entries)\n",
    "\n",
    "forest_df = pd.DataFrame(entries)\n",
    "\n",
    "separate_results = []\n",
    "for file in Path(\"../best_distances/\").iterdir():\n",
    "    if \"RSIF\" in file.stem and (\"ovarian\" in file.stem or \"breast\" in file.stem or \"rosmap\" in file.stem):\n",
    "        get_single_result(file, separate_results)\n",
    "\n",
    "\n",
    "\n",
    "for file in Path(\"../best_distances/\").iterdir():\n",
    "    if \"LOF\" in file.stem:\n",
    "        df = pd.read_csv(file)\n",
    "        entries = []\n",
    "        distances = []\n",
    "        entry = {'clf': \"LOF_tuned\"}\n",
    "        for row in df.iterrows():\n",
    "            i, row = row\n",
    "            distances.append((row['auc'], row[\"metric\"]))\n",
    "            if (i+1) % 3 == 0:\n",
    "                distances.sort(key=lambda x: x[0], reverse=True)\n",
    "                entry['distances_LOF_tuned'] = [d[1] for d in distances]\n",
    "                entry['auc'] = distances[0][0]\n",
    "                entry['fold'] = row['fold_id']\n",
    "                entry['dataset_name'] = row['dataset']\n",
    "                entries.append(entry)\n",
    "                entry = {'clf': \"LOF_tuned\"}\n",
    "                distances = []\n",
    "\n",
    "lof_df = pd.DataFrame(entries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = pd.DataFrame(separate_results) # FOR MIXED DATA\n",
    "sep['dataset_name'] = sep. dataset_name.map(lambda x: x[:-1])\n",
    "sep = sep.groupby(['fold', 'dataset_name']).auc.max().reset_index()\n",
    "sep['clf'] = \"RSIF_tuned\"\n",
    "sep[\"distances_RSIF_tuned\"] = \"not implemented\"\n",
    "\n",
    "forest_df = pd.concat([forest_df,sep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper = pd.concat([forest_df, lof_df], axis=0)\n",
    "df_hyper_pivot = pd.pivot_table(df_hyper, columns=[\"clf\"], values=\"auc\", index=['dataset_name', 'fold']).reset_index()\n",
    "for dist_name in [ \"distances_LOF_tuned\", 'distances_ISF_tuned', \"distances_RSIF_tuned\"]:\n",
    "    df_hyper_pivot[dist_name] = df_hyper[~df_hyper[dist_name].isna()].sort_values(by=['dataset_name', 'fold'])[dist_name].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_final_results, df_hyper_pivot.loc[:, [\"distances_LOF_tuned\",\"distances_ISF_tuned\", \"ISF_tuned\", \"LOF_tuned\", \"RSIF_tuned\", \"distances_RSIF_tuned\"]]], axis=1).dropna()\n",
    "df_final['dataset_name'] = df_final['dataset_name'].str.slice(stop=10) + \"_\" + df_final['dataset_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in [\"RSIF\", \"ISF\", \"LOF\"]:\n",
    "    df_final[\"diff_to_if\"] = df_final[algorithm] - df_final['IForest']\n",
    "    fig = px.scatter(df_final, x = f\"{algorithm}_tuned\", y = algorithm , color = \"diff_to_if\", hover_data= [f\"distances_{algorithm}_tuned\", \"fold\"], facet_col=\"dataset_name\", \n",
    "                facet_col_wrap=5, height=2000, title = f\"Performance on validation vs test set for {algorithm}\",\n",
    "                labels = {\"diff_to_if\": \"Difference to IForest\", f\"{algorithm}_tuned\": \"Performance on validation set\", algorithm: \"Performance on test set\"},\n",
    "                range_color = (-1,1),\n",
    "                color_continuous_scale = [(0, \"red\"), (0.5, \"red\"), (0.5,\"green\"), (1, \"green\")])\n",
    "\n",
    "    fig = fig.add_shape(type= 'line',\n",
    "                yref= 'y', y0=0, y1= 1,\n",
    "                xref= 'x', x0=0, x1= 1, row=\"all\", col=\"all\",\n",
    "                line=dict(\n",
    "                    width=1,\n",
    "                    dash=\"dashdot\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.write_html(FIG_DIR / f\"{algorithm}_distance_search.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing how the projection changes as we play with pairs that create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risf.distance_functions import EuclideanDist, ProperEuclideanDist\n",
    "from risf.distance import TrainDistanceMixin\n",
    "\n",
    "def generate_data(n_dim, proper_euclidean=False):\n",
    "    X = np.random.normal(0, 3, size=(100, n_dim))\n",
    "\n",
    "    if proper_euclidean:\n",
    "        entire_distance = TrainDistanceMixin(ProperEuclideanDist())\n",
    "    else:\n",
    "        entire_distance = TrainDistanceMixin(EuclideanDist())\n",
    "    \n",
    "    entire_distance.precompute_distances(X, n_jobs=-1)\n",
    "\n",
    "    df = pd.DataFrame(entire_distance.distance_matrix)\n",
    "    df = df.unstack().reset_index()\n",
    "    df.columns=['x','y','V']\n",
    "    df = df[df.x != df.y]\n",
    "    df = df.sort_values(by=['V'], ascending=True)\n",
    "\n",
    "    return X, entire_distance, df, df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_PLOTS_PATH = FIG_DIR / \"projection_plots\"\n",
    "PROJECTION_PLOTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "def rand_jitter(arr):\n",
    "    stdev = .055 * (max(arr) - min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "\n",
    "def generate_plot(X, projection, x, y, n_dim = 1, old_df_to_plot = None, v = None):\n",
    "    columns = [f\"x{i}\" for i in range(n_dim)]\n",
    "    df_to_plot = pd.concat([pd.DataFrame(X, columns=columns), pd.DataFrame(projection, columns = [\"projection\"])], axis=1)\n",
    "    df_to_plot[\"used\"] = \"red\"\n",
    "    df_to_plot.iloc[x.item(), n_dim + 1] = \"blue\"\n",
    "    df_to_plot.iloc[y.item(), n_dim + 1] = \"blue\"\n",
    "\n",
    "    if n_dim == 1:\n",
    "        ax[col][row].scatter(df_to_plot.x0, df_to_plot.projection, c=df_to_plot.used, alpha=0.4)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        df_to_plot['distance_p_q'] = v\n",
    "        if old_df_to_plot is not None:\n",
    "            return pd.concat([old_df_to_plot, df_to_plot], axis = 0)\n",
    "        return df_to_plot\n",
    "\n",
    "\n",
    "for proper_euclidean in [True, False]:\n",
    "    for n_dim in [1, 2, 3]:\n",
    "        df_to_plot = None\n",
    "        X, entire_distance, df, n_rows = generate_data(n_dim, proper_euclidean)\n",
    "        if n_dim == 1:\n",
    "            fig, ax = plt.subplots(7, 3, figsize=(35, 25))\n",
    "        row = 0\n",
    "        col = 0\n",
    "        for idx in range(0, n_rows + 1, n_rows//20):\n",
    "            if idx == n_rows:\n",
    "                idx = n_rows - 1\n",
    "            x, y, v = df.iloc[idx]\n",
    "            x, y = np.array(int(x)), np.array(int(y))\n",
    "\n",
    "            projection = entire_distance.project(np.arange(X.shape[0]), x, y)\n",
    "            df_to_plot = generate_plot(X, projection, x, y, n_dim, df_to_plot, v)\n",
    "\n",
    "            if n_dim == 1:\n",
    "                ax[col][row].set_title(f\"Projection value when distance between p and q  is {v}\")\n",
    "                ax[col][row].set_xlabel(\"x\")\n",
    "                ax[col][row].set_ylabel('projection value')\n",
    "\n",
    "            row += 1\n",
    "            if row == 3:\n",
    "                row = 0\n",
    "                col += 1\n",
    "        \n",
    "        if n_dim == 1:\n",
    "            fig.tight_layout()\n",
    "            prefix = \"proper_eulicdean\" if proper_euclidean else \"approx_euclidean\"\n",
    "            plt.savefig(PROJECTION_PLOTS_PATH / f\"{n_dim}_dim_{prefix}.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        if n_dim == 2:\n",
    "            df_to_plot['used'] = [0.5 if x == \"red\" else 3 for x in df_to_plot['used']] \n",
    "            fig = px.scatter(df_to_plot, x = \"x0\", y = \"x1\" , color = \"projection\", facet_col=\"distance_p_q\", facet_col_wrap=5, height=2000, title = f\"Projection values depending on p and q selected\", size=\"used\", symbol=\"used\", range_color=(-100,100))\n",
    "            fig.write_html(PROJECTION_PLOTS_PATH / f\"2_dim_{prefix}.html\")\n",
    "\n",
    "        if n_dim == 3:\n",
    "            df_to_plot['used'] = [0.5 if x == \"red\" else 3 for x in df_to_plot['used']] \n",
    "            for i, v in enumerate(df_to_plot.distance_p_q.unique()):\n",
    "                df = df_to_plot[df_to_plot.distance_p_q == v]\n",
    "                fig = px.scatter_3d(df, x = \"x0\", y = \"x1\", z=\"x2\", title = f\"Projection when distance between p and q is equal to {v}\", color='projection', symbol=\"used\", size='used', range_color=(-100,100))\n",
    "                fig.write_html(PROJECTION_PLOTS_PATH / f\"3_dim_{i}_{prefix}.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random-isolation-similarity-forest-bqxKmFq8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
