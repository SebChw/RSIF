{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Experiments comparing Random Isolation Similarity Forest to other outlier (anomaly) detection algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from data.data_getter import get_numerical_datasets\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyod.utils.utility import precision_n_scores\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from notebooks.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use different outlier detection algorithms to compare to RISF:\n",
    "* LOF\n",
    "* ECOD\n",
    "* Isolation Forest\n",
    "* HBOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure AUC (as a binary classification task of being an outlier) and processing time. We can show plots for every algorithm and the top-N feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_names = ['ECOD', 'LOF', 'IForest', 'HBOS', 'RISF']\n",
    "results = {x: {} for x in clfs_names}\n",
    "resultsY = results.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer(timer_type=\"long_running\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets (outer loop): 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "datasets_loop = tqdm(get_numerical_datasets(),\n",
    "                     desc=\"Datasets (outer loop)\", position=0)\n",
    "algorithms_loop = tqdm(\n",
    "    clfs_names, desc=\" Algorithms (inner loop)\", position=1, leave=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only X known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23_WPBC.npz: : 24it [1:32:34, 231.44s/it]        \n"
     ]
    }
   ],
   "source": [
    "for data in datasets_loop:\n",
    "    datasets_loop.set_description(data['name'])\n",
    "    for clf_name in algorithms_loop:\n",
    "        algorithms_loop.set_description(clf_name)\n",
    "        clf = new_clf(clf_name, SEED)\n",
    "        timer.start()\n",
    "        clf.fit(data['X_train'])\n",
    "        timer.stop()\n",
    "        train_time = timer.time_sec\n",
    "\n",
    "        # get the prediction labels and outlier scores of the training and tests  data\n",
    "        y_train_pred = clf.decision_scores_\n",
    "\n",
    "        timer.start()\n",
    "        y_test_pred = clf.decision_function(data['X_test'])\n",
    "        timer.stop()\n",
    "        test_time = timer.time_sec\n",
    "\n",
    "        if np.isnan(y_train_pred).any():\n",
    "            results[clf_name][data['name']] = (np.nan, np.nan, np.nan, np.nan,\n",
    "                                               np.nan, np.nan, np.nan, np.nan)\n",
    "            # AUC/ROC, Rank@N for train,test ; fit/test Time\n",
    "            continue\n",
    "\n",
    "        roc_train = np.round(roc_auc_score(\n",
    "            data['y_train'], y_train_pred), decimals=4)\n",
    "        ap_train = np.round(average_precision_score(\n",
    "            data['y_train'], y_train_pred), decimals=4)\n",
    "        roc_test = np.round(roc_auc_score(\n",
    "            data['y_test'], y_test_pred), decimals=4)\n",
    "        ap_test = np.round(average_precision_score(\n",
    "            data['y_test'], y_test_pred), decimals=4)\n",
    "\n",
    "        results[clf_name][data['name']] = (roc_train, ap_train,\n",
    "                                           roc_test, ap_test,\n",
    "                                           train_time, test_time)\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_pickle('../results/numerical_temporary.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_pickle('../results/numerical_selected.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training test provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets (outer loop): 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "datasets_loop = tqdm(get_numerical_datasets(),\n",
    "                     desc=\"Datasets (outer loop)\", position=0)\n",
    "algorithms_loop = tqdm(\n",
    "    clfs_names, desc=\" Algorithms (inner loop)\", position=1, leave=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23_WPBC.npz: : 24it [2:43:55, 409.83s/it]        \n"
     ]
    }
   ],
   "source": [
    "for data in datasets_loop:\n",
    "    datasets_loop.set_description(data['name'])\n",
    "    for clf_name in algorithms_loop:\n",
    "        algorithms_loop.set_description(clf_name)\n",
    "        clf = new_clf(clf_name, SEED)\n",
    "        timer.start()\n",
    "        clf.fit(data['X_train'], data['y_train'])\n",
    "        timer.stop()\n",
    "        train_time = timer.time_sec\n",
    "\n",
    "        # get the prediction labels and outlier scores of the training and tests  data\n",
    "        y_train_pred = clf.decision_scores_\n",
    "\n",
    "        timer.start()\n",
    "        y_test_pred = clf.decision_function(data['X_test'])\n",
    "        timer.stop()\n",
    "        test_time = timer.time_sec\n",
    "\n",
    "        if np.isnan(y_train_pred).any():\n",
    "            resultsY[clf_name][data['name']] = (np.nan, np.nan, np.nan, np.nan,\n",
    "                                                np.nan, np.nan, np.nan, np.nan)\n",
    "            # AUC/ROC, Rank@N for train,test ; fit/test Time\n",
    "            continue\n",
    "\n",
    "        roc_train = np.round(roc_auc_score(\n",
    "            data['y_train'], y_train_pred), decimals=4)\n",
    "        ap_train = np.round(average_precision_score(\n",
    "            data['y_train'], y_train_pred), decimals=4)\n",
    "        roc_test = np.round(roc_auc_score(\n",
    "            data['y_test'], y_test_pred), decimals=4)\n",
    "        ap_test = np.round(average_precision_score(\n",
    "            data['y_test'], y_test_pred), decimals=4)\n",
    "\n",
    "        resultsY[clf_name][data['name']] = (roc_train, ap_train,\n",
    "                                            roc_test, ap_test,\n",
    "                                            train_time, test_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(resultsY)\n",
    "df.to_pickle('../results/numerical_y.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b56e3c97de72a7673ee0fcfb94642a98e5ba521673e7fa53275c57ef5fa8664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
