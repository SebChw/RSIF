{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Experiments comparing Random Isolation Similarity Forest to other outlier (anomaly) detection algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from data.data_getter import get_numerical_datasets\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyod.utils.utility import precision_n_scores\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from notebooks.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use different outlier detection algorithms to compare to RISF:\n",
    "* LOF\n",
    "* ECOD\n",
    "* Isolation Forest\n",
    "* HBOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure AUC (as a binary classification task of being an outlier) and processing time. We can show plots for every algorithm and the top-N feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_names = ['ECOD', 'LOF', 'IForest', 'HBOS', 'RISF']\n",
    "results = {x: {} for x in clfs_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer(timer_type=\"long_running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets (outer loop): 0it [01:09, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets_loop = tqdm(get_numerical_datasets(), desc=\"Datasets (outer loop)\", position=0)\n",
    "algorithms_loop = tqdm(clfs_names, desc=\" Algorithms (inner loop)\", position=1, leave=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23_WPBC.npz: : 23it [55:53, 145.80s/it]          \n"
     ]
    }
   ],
   "source": [
    "for data in datasets_loop:\n",
    "    datasets_loop.set_description(data['name'])\n",
    "    for clf_name in algorithms_loop:\n",
    "        algorithms_loop.set_description(clf_name)\n",
    "        clf = new_clf(clf_name, SEED)\n",
    "        timer.start()\n",
    "        clf.fit(data['X_train'])\n",
    "        timer.stop()\n",
    "        train_time = timer.time_sec\n",
    "        \n",
    "        # get the prediction labels and outlier scores of the training and tests  data\n",
    "        if clf_name == 'RISF': # other libs return sklearn UndefinedMetricWarning from predicting th train data\n",
    "            y_train_pred = clf.predict(data['X_train']) # binary labels (0: inliers, 1: outliers)\n",
    "        else:\n",
    "            y_train_pred = clf.labels_\n",
    "        \n",
    "        timer.start()\n",
    "        y_test_pred = clf.predict(data['X_test'])\n",
    "        timer.stop()\n",
    "        test_time = timer.time_sec\n",
    "\n",
    "        if np.isnan(y_train_pred).any():\n",
    "            results[clf_name][data['name']] = (np.nan, np.nan, np.nan, np.nan, \n",
    "                                               np.nan, np.nan, np.nan, np.nan) \n",
    "                                               # AUC/ROC, Rank@N for train,test ; fit/test Time\n",
    "            continue\n",
    "        \n",
    "        roc_train=np.round(roc_auc_score(data['y_train'], y_train_pred), decimals=4)\n",
    "        precision_train=np.round(precision_score(data['y_train'], y_train_pred), decimals=4)\n",
    "        recall_train=np.round(recall_score(data['y_train'], y_train_pred), decimals=4)\n",
    "        roc_test=np.round(roc_auc_score(data['y_test'], y_test_pred), decimals=4)\n",
    "        precision_test=np.round(precision_score(data['y_test'], y_test_pred), decimals=4)\n",
    "        recall_test=np.round(recall_score(data['y_test'], y_test_pred), decimals=4)\n",
    "\n",
    "        results[clf_name][data['name']] = (roc_train, precision_train, recall_train,\n",
    "                                           roc_test, precision_test, recall_test,\n",
    "                                           train_time, test_time)\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_pickle('../results/numerical_temporary.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_pickle('../results/numerical_selected.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04_breastw.npz: : 3it [10:31, 210.34s/it]   \n"
     ]
    }
   ],
   "source": [
    "for data in datasets_loop:\n",
    "    datasets_loop.set_description(data['name'])\n",
    "    for clf_name in algorithms_loop:\n",
    "        algorithms_loop.set_description(clf_name)\n",
    "        clf = new_clf(clf_name, SEED)\n",
    "        timer.start()\n",
    "        clf.fit(data['X_train'], data['y_train'])\n",
    "        timer.stop()\n",
    "        train_time = timer.time_sec\n",
    "        \n",
    "        # get the prediction labels and outlier scores of the training and tests  data\n",
    "        if clf_name == 'RISF': # other libs return sklearn UndefinedMetricWarning from predicting th train data\n",
    "            y_train_pred = clf.predict(data['X_train']) # binary labels (0: inliers, 1: outliers)\n",
    "        else:\n",
    "            y_train_pred = clf.labels_\n",
    "        \n",
    "        timer.start()\n",
    "        y_test_pred = clf.predict(data['X_test'])\n",
    "        timer.stop()\n",
    "        test_time = timer.time_sec\n",
    "\n",
    "        if np.isnan(y_train_pred).any():\n",
    "            results[clf_name][data['name']] = (np.nan, np.nan, np.nan, np.nan, \n",
    "                                               np.nan, np.nan, np.nan, np.nan) \n",
    "                                               # AUC/ROC, Rank@N for train,test ; fit/test Time\n",
    "            continue\n",
    "        \n",
    "        roc_train=np.round(roc_auc_score(data['y_train'], y_train_pred), decimals=4)\n",
    "        precision_train=np.round(precision_score(data['y_train'], y_train_pred), decimals=4)\n",
    "        recall_train=np.round(recall_score(data['y_train'], y_train_pred), decimals=4)\n",
    "        roc_test=np.round(roc_auc_score(data['y_test'], y_test_pred), decimals=4)\n",
    "        precision_test=np.round(precision_score(data['y_test'], y_test_pred), decimals=4)\n",
    "        recall_test=np.round(recall_score(data['y_test'], y_test_pred), decimals=4)\n",
    "\n",
    "        results[clf_name][data['name']] = (roc_train, precision_train, recall_train,\n",
    "                                           roc_test, precision_test, recall_test,\n",
    "                                           train_time, test_time)\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_pickle('../results/numerical_temporary.pkl')\n",
    "    if 'breast' in data['name']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b56e3c97de72a7673ee0fcfb94642a98e5ba521673e7fa53275c57ef5fa8664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
